{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch-ONNX-TensorRT Workflow example\n",
    "\n",
    "This Jupyter notebook serves as a guide to understand how the workflow implemented in the experiments of this work functions. The workflow itself is described in the following image:\n",
    "\n",
    "![Workflow.](/outputs/img_readme/TensorRT_workflow.png)\n",
    "\n",
    "In this case, we will use the described workflow in a specific example: image classification using the ImageNet-1k dataset with the MobileNetV2 model.\n",
    "\n",
    "## Base model\n",
    "\n",
    "To obtain the base model, we use the pre-trained models on ImageNet-1k provided by PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.15.2', \"mobilenet_v2\", weights=f'MobileNet_V2_Weights.DEFAULT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some constants that will be used by our model throughout the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTES\n",
    "BATCH_SIZE = 1\n",
    "C = 3 # number of channels of the input image\n",
    "H = 224 # heigh of the input image\n",
    "W = 224 # width of the input image\n",
    "NETWORK = 'mobilenet' # mobiletv2\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX: Model Conversion\n",
    "\n",
    "To convert the model from `.pt` to `.onnx`, we use the code described in `onnx_transform.py` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run onnx_transform.py --weights weights/best.pth --pretrained --network $NETWORK --input_shape $BATCH_SIZE $C $H $W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has created the ONNX model, saved in `weights/best.onn`. Now, we need to create the TensorRT Application and optimize the model with it.\n",
    "\n",
    "## TensorRT Application\n",
    "\n",
    "To create the TensorRT Application and optimize the model in ONNX format, we use the code in build_trt.py. This code calls `utils/engine.py`, which describes the TensorRT Application. In `utils/engine.py`, you can make further changes to experiment with different types of optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./build_trt.py --weights weights/best.onnx  --fp32 --input_shape $BATCH_SIZE $C $H $W --engine_name best_fp32.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run build_trt.py --weights weights/best.onnx  --fp16 --input_shape $BATCH_SIZE $C $H $W --engine_name best_fp16.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen('rm -r outputs/cache > /dev/null 2>&1', shell=True)\n",
    "%run build_trt.py --weights weights/best.onnx  --int8 --input_shape $BATCH_SIZE $C $H $W --engine_name best_int8.engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more specific optimizations, refer to the wiki... (a wiki will be available soon)\n",
    "\n",
    "The optimized models are stored in the `weights` folder.\n",
    "\n",
    "## Running Phase: Inference using optimized model\n",
    "\n",
    "First, we ensure that the system has a GPU capable of using CUDA. Then, we load the optimized models and deserialize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.engine import TRTModule\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if not gpu_available:\n",
    "    print('CUDA is not available.')\n",
    "else:\n",
    "    print('CUDA is available.')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if gpu_available else \"cpu\")\n",
    "\n",
    "engine_path_1 = os.path.join(current_directory,\"weights/best_fp32.engine\")\n",
    "engine_path_2 = os.path.join(current_directory,\"weights/best_fp16.engine\")\n",
    "engine_path_3 = os.path.join(current_directory,\"weights/best_int8.engine\")\n",
    "Engine_fp32 = TRTModule(engine_path_1,device)\n",
    "Engine_fp16 = TRTModule(engine_path_2,device)\n",
    "Engine_int8 = TRTModule(engine_path_3,device)\n",
    "Engine_fp32.set_desired(['outputs'])\n",
    "Engine_fp16.set_desired(['outputs'])\n",
    "Engine_int8.set_desired(['outputs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the ImageNet-1k validation dataset, available [here](https://drive.google.com/drive/folders/1xHxI_S03Wjh56g2W8_1pNzXCTiKheqYY?usp=sharing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_loader import val_data_loader\n",
    "val_loader = val_data_loader(os.path.join(current_directory,'datasets/dataset_val/val'), batch_size=BATCH_SIZE, workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate the models using the validate function described below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import AverageMeter, accuracy\n",
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "def validate(model_version, val_loader, model, criterion=nn.CrossEntropyLoss().to(device)):\n",
    "    batch_time_all = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Calculate 10% of total batches\n",
    "    warmup_batches = int(0.1 * len(val_loader))\n",
    "    \n",
    "    # Initialize the maximum and minimum processing time after warm-up\n",
    "    max_time_all = 0\n",
    "    min_time_all = float('inf')\n",
    "\n",
    "    num_batches_to_process = int(1 * len(val_loader))\n",
    "\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if i >= num_batches_to_process:\n",
    "            break\n",
    "\n",
    "        target = target.to(device)\n",
    "        start_all = time.time() # start time, moving data to gpu\n",
    "        input = input.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            output_cpu = output.cpu() # needed to get the time from gpu to cpu\n",
    "            all_time = (time.time() - start_all) * 1000  # Convert to milliseconds / time when the result pass to cpu again \n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1[0], input.size(0))\n",
    "        top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time in milliseconds and ignore first 10% batches\n",
    "        if i >= warmup_batches:\n",
    "            batch_time_all.update(all_time)\n",
    "            max_time_all = max(max_time_all, all_time)\n",
    "            min_time_all = min(min_time_all, all_time)\n",
    "       \n",
    "    print(\"|  Model          | Latency avg (ms)| Latency max (ms) | accuracy (Prec@1) (%)|accuracy (Prec@5) (%)|\")\n",
    "    print(\"|-----------------|-----------------|------------------|----------------------|---------------------|\")\n",
    "    print(\"| {:<15} | {:<15.1f} | {:<15.1f} |  {:<20.2f} | {:<20.2f} | \".format(\n",
    "        model_version,\n",
    "        batch_time_all.avg, max_time_all,\n",
    "        top1.avg, top5.avg))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate('Base Model', val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT fp32 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate('TRT fp32', val_loader, Engine_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT fp16 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate('TRT fp32', val_loader, Engine_fp16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRT int8 Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate('TRT fp32', val_loader, Engine_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific usage of `build_trt.py` and `onnx_transform.py`, refer to the wiki.\n",
    "\n",
    "To generate the tables presented with metrics such as latency, throughput, parameters, weights, etc., refer to the `experiments/main` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
