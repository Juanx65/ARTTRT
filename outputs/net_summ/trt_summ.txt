        Layer 0    | Reformatting CopyNode for Input Tensor 0 to /conv1/Conv + /relu/Relu [Op: Reformat]
            {images [dtype=float32, shape=(1, 3, 224, 224), Format: Row major linear FP32]}
             -> {Reformatted Input Tensor 0 to /conv1/Conv + /relu/Relu [dtype=float16, shape=(1, 3, 224, 224), Format: Channel major FP16 format where channel % 4 == 0]}
    
        Layer 1    | /conv1/Conv + /relu/Relu [Op: CaskConvolution]
            {Reformatted Input Tensor 0 to /conv1/Conv + /relu/Relu [dtype=float16, shape=(1, 3, 224, 224), Format: Channel major FP16 format where channel % 4 == 0]}
             -> {/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 112, 112), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 2    | /maxpool/MaxPool [Op: CaskPooling]
            {/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 112, 112), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/maxpool/MaxPool_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 3    | /layer1/layer1.0/conv1/Conv + /layer1/layer1.0/relu/Relu [Op: CaskConvolution]
            {/maxpool/MaxPool_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer1/layer1.0/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 4    | /layer1/layer1.0/conv2/Conv + /layer1/layer1.0/Add + /layer1/layer1.0/relu_1/Relu [Op: CaskConvolution]
            {/layer1/layer1.0/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0],
             /maxpool/MaxPool_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer1/layer1.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 5    | /layer1/layer1.1/conv1/Conv + /layer1/layer1.1/relu/Relu [Op: CaskConvolution]
            {/layer1/layer1.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer1/layer1.1/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 6    | /layer1/layer1.1/conv2/Conv + /layer1/layer1.1/Add + /layer1/layer1.1/relu_1/Relu [Op: CaskConvolution]
            {/layer1/layer1.1/relu/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0],
             /layer1/layer1.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer1/layer1.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 7    | /layer2/layer2.0/conv1/Conv + /layer2/layer2.0/relu/Relu [Op: CaskConvolution]
            {/layer1/layer1.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer2/layer2.0/relu/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 8    | /layer2/layer2.0/downsample/downsample.0/Conv [Op: CaskConvolution]
            {/layer1/layer1.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 64, 56, 56), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer2/layer2.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 9    | /layer2/layer2.0/conv2/Conv + /layer2/layer2.0/Add + /layer2/layer2.0/relu_1/Relu [Op: CaskConvolution]
            {/layer2/layer2.0/relu/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0],
             /layer2/layer2.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer2/layer2.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 10   | /layer2/layer2.1/conv1/Conv + /layer2/layer2.1/relu/Relu [Op: CaskConvolution]
            {/layer2/layer2.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer2/layer2.1/relu/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 11   | /layer2/layer2.1/conv2/Conv + /layer2/layer2.1/Add + /layer2/layer2.1/relu_1/Relu [Op: CaskConvolution]
            {/layer2/layer2.1/relu/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0],
             /layer2/layer2.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer2/layer2.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 12   | /layer3/layer3.0/conv1/Conv + /layer3/layer3.0/relu/Relu [Op: CaskConvolution]
            {/layer2/layer2.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer3/layer3.0/relu/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 13   | /layer3/layer3.0/downsample/downsample.0/Conv [Op: CaskConvolution]
            {/layer2/layer2.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 128, 28, 28), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer3/layer3.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 14   | /layer3/layer3.0/conv2/Conv + /layer3/layer3.0/Add + /layer3/layer3.0/relu_1/Relu [Op: CaskConvolution]
            {/layer3/layer3.0/relu/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0],
             /layer3/layer3.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer3/layer3.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 15   | /layer3/layer3.1/conv1/Conv + /layer3/layer3.1/relu/Relu [Op: CaskConvolution]
            {/layer3/layer3.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer3/layer3.1/relu/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 16   | /layer3/layer3.1/conv2/Conv + /layer3/layer3.1/Add + /layer3/layer3.1/relu_1/Relu [Op: CaskConvolution]
            {/layer3/layer3.1/relu/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0],
             /layer3/layer3.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer3/layer3.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 17   | /layer4/layer4.0/conv1/Conv + /layer4/layer4.0/relu/Relu [Op: CaskConvolution]
            {/layer3/layer3.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer4/layer4.0/relu/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 18   | /layer4/layer4.0/downsample/downsample.0/Conv [Op: CaskConvolution]
            {/layer3/layer3.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 256, 14, 14), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer4/layer4.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 19   | /layer4/layer4.0/conv2/Conv + /layer4/layer4.0/Add + /layer4/layer4.0/relu_1/Relu [Op: CaskConvolution]
            {/layer4/layer4.0/relu/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0],
             /layer4/layer4.0/downsample/downsample.0/Conv_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer4/layer4.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 20   | /layer4/layer4.1/conv1/Conv + /layer4/layer4.1/relu/Relu [Op: CaskConvolution]
            {/layer4/layer4.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer4/layer4.1/relu/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 21   | /layer4/layer4.1/conv2/Conv + /layer4/layer4.1/Add + /layer4/layer4.1/relu_1/Relu [Op: CaskConvolution]
            {/layer4/layer4.1/relu/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0],
             /layer4/layer4.0/relu_1/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/layer4/layer4.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 22   | /avgpool/GlobalAveragePool [Op: CaskPooling]
            {/layer4/layer4.1/relu_1/Relu_output_0 [dtype=float16, shape=(1, 512, 7, 7), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/avgpool/GlobalAveragePool_output_0 [dtype=float16, shape=(1, 512, 1, 1), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 23   | /fc/Gemm [Op: CaskGemmConvolution]
            {/avgpool/GlobalAveragePool_output_0 [dtype=float16, shape=(1, 512, 1, 1), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {/fc/Gemm_out_tensor [dtype=float16, shape=(1, 1000, 1, 1), Format: Channel major FP16 format where channel % 8 == 0]}
    
        Layer 24   | Reformatting CopyNode for Input Tensor 0 to reshape_after_/fc/Gemm [Op: NoOp]
            {/fc/Gemm_out_tensor [dtype=float16, shape=(1, 1000, 1, 1), Format: Channel major FP16 format where channel % 8 == 0]}
             -> {Reformatted Input Tensor 0 to reshape_after_/fc/Gemm [dtype=float16, shape=(1, 1000, 1, 1), Format: Row major linear FP16 format]}
    
        Layer 25   | reshape_after_/fc/Gemm [Op: NoOp]
            {Reformatted Input Tensor 0 to reshape_after_/fc/Gemm [dtype=float16, shape=(1, 1000, 1, 1), Format: Row major linear FP16 format]}
             -> {Reformatted Output Tensor 0 to reshape_after_/fc/Gemm [dtype=float16, shape=(1, 1000), Format: Row major linear FP16 format]}
    
        Layer 26   | Reformatting CopyNode for Output Tensor 0 to reshape_after_/fc/Gemm [Op: Reformat]
            {Reformatted Output Tensor 0 to reshape_after_/fc/Gemm [dtype=float16, shape=(1, 1000), Format: Row major linear FP16 format]}
             -> {outputs [dtype=float32, shape=(1, 1000), Format: Row major linear FP32]}
